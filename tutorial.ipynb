{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial, you can learn about OpenAI Assistant and explore its capabilities further.\n",
    "\n",
    "## table of content\n",
    ">- What is OpenAI assistant?\n",
    ">- Leveraging LangChain to utilize OpenAI Assistant capabilities.\n",
    ">- Combining OpenAI Assistant and LangChain tool to extend capabilities in the context of OpenAI Assistant.\n",
    "\n",
    "## What is OpenAI assistant?\n",
    "The Assistants API enables the creation of AI assistants directly within your applications. These assistants are equipped with instructions and can utilize various models, tools, and knowledge to provide responses to user inquiries. Currently, the Assistants API supports three categories of tools: \n",
    "1. Code Interpreter\n",
    "2. Retrieval\n",
    "3. Function Calling\n",
    "\n",
    "For more details follow [LangChain](https://python.langchain.com/docs/modules/agents/agent_types/openai_assistants) and [OpenAI assistants](https://platform.openai.com/docs/assistants/overview)\n",
    "\n",
    "## Leveraging LangChain to utilize OpenAI Assistant capabilities\n",
    "Let's craft our own AI assistant!üöÄ\n",
    "\n",
    "### Project info\n",
    "Let's create a research assistant! `ScholarlySphere` is a Document Analysis and Search assistant.\n",
    "Users can upload PDF or text files containing documents or research papers. `ScholarlySphere` can then extract key information, such as keywords, topics, or main points, and use search engines to find relevant information to answer user questions based on the content of the uploaded files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import some packages\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable # import OpenAIAssistantRunnable from langchain\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have an openAI API key\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need an instruction. An instruction is how the Assistant and model should behave or respond.\n",
    "instructions = \"\"\"\n",
    "    You are a scholarly expert, you have the skill to extract crucial details from academic papers provided \n",
    "    for you and utilize online resources to find similar papers. Your expertise enables you\n",
    "    to generate comprehensive outputs based on user desires.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# What we need in the output?\n",
    "file_prompt = \"\"\"\n",
    "    Write a detailed summary of the input paper, synthesizing its key findings, methodologies, and \n",
    "    implications, providing overview for reference and comprehension purposes.\"\"\"\n",
    "\n",
    "\n",
    "online_prompt = \"\"\"\n",
    "    Give structured JSON file for similar papers that you found contains: paper_title, authors_names, \n",
    "    publication_date, link_of_paper and the conference_or_journal_name where it was published.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filrst we need ti initiate OpenAI client instance\n",
    "client = OpenAI()\n",
    "\n",
    "# Upload a file with an \"assistants\" purpose\n",
    "file = client.files.create(\n",
    "  file=open(\"attention_is_all_you_need.pdf\", \"rb\"),\n",
    "  purpose='assistants'\n",
    ")\n",
    "\n",
    "# Add the file to the assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Document Analysis and Search assistant\",\n",
    "  instructions=instructions,\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  tools=[{\"type\": \"retrieval\"}], # \n",
    "  file_ids=[file.id]\n",
    ")\n",
    "\n",
    "# create a thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=file_prompt,\n",
    "  file_ids=[file.id]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_Ntd8pntQfbchrtwBxf5YubmV', assistant_id='asst_ilzVq2Zpu4cY9h4x0zA039BP', content=[MessageContentText(text=Text(annotations=[TextAnnotationFileCitation(end_index=1163, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='Abstract\\n\\n\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture the Transformer\\nbased solely on attention mechanisms dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task improving over the existing best results including\\nensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data'), start_index=1153, text='„Äê9‚Ä†source„Äë', type='file_citation'), TextAnnotationFileCitation(end_index=1939, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='1 Introduction\\n\\n\\nRecurrent neural networks long short-term memory [13] and gated recurrent [7] neural networks\\nin particular have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35 2 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38 24 15].\\n\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time they generate a sequence of hidden\\nstates ht\\n as a function of the previous hidden state ht‚àí1 \\nand the input for position t. This inherently\\nsequential nature precludes parallelization within training examples which becomes critical at longer\\nsequence lengths as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32] while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation however remains.\\n\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2 19]. In all but a few cases [27] however such attention mechanisms\\nare used in conjunction with a recurrent network.\\n\\n\\nIn this work we propose the Transformer a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n\\n2 Background\\n\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16] ByteNet [18] and ConvS2S [9] all of which use convolutional neural networks as basic building\\nblock computing hidden representations in parallel for all input and output positions. In these models\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n\\n\\nSelf-attention sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension abstractive summarization\\ntextual entailment and learning task-independent sentence representations [4 27 28 22].\\n\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\n\\n\\nTo the best of our knowledge however the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections we will describe the Transformer motivate\\nself-attention and discuss its advantages over models such as [17 18] and [9].\\n\\n\\n3 Model Architecture\\n\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5 2 35].\\nHere the encoder maps an input sequence of symbol representations (x1\\n ... xn\\n) to a sequence\\nof continuous representations z = (z1\\n ... zn\\n).  Given z the decoder then generates an output\\nsequence (y1\\n ... ym\\n) of symbols one element at a time. At each step the model is auto-regressive\\n[10] consuming the previously generated symbols as additional input when generating the next.\\n\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise fully\\nconnected layers for both the encoder and decoder shown in the left and right halves of Figure 1\\nrespectively.\\n\\n\\n3.1  Encoder and Decoder Stacks\\n\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism and the second is a simple position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers followed by layer normalization [1]. That is the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)) where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections all sub-layers in the model as well as the embedding\\nlayers produce outputs of dimension dmodel \\n= 512.\\n\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer the decoder inserts a third sub-layer which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder we employ residual connections\\naround each of the sub-layers followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking combined with fact that the output embeddings are offset by one position ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i'), start_index=1928, text='„Äê10‚Ä†source„Äë', type='file_citation'), TextAnnotationFileCitation(end_index=1950, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='2 Background\\n\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16] ByteNet [18] and ConvS2S [9] all of which use convolutional neural networks as basic building\\nblock computing hidden representations in parallel for all input and output positions. In these models\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2'), start_index=1939, text='„Äê11‚Ä†source„Äë', type='file_citation'), TextAnnotationFileCitation(end_index=2938, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='3 Model Architecture\\n\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5 2 35].\\nHere the encoder maps an input sequence of symbol representations (x1\\n ... xn\\n) to a sequence\\nof continuous representations z = (z1\\n ... zn\\n).  Given z the decoder then generates an output\\nsequence (y1\\n ... ym\\n) of symbols one element at a time. At each step the model is auto-regressive\\n[10] consuming the previously generated symbols as additional input when generating the next.\\n\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise fully\\nconnected layers for both the encoder and decoder shown in the left and right halves of Figure 1\\nrespectively.\\n\\n\\n3.1  Encoder and Decoder Stacks\\n\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism and the second is a simple position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers followed by layer normalization [1]. That is the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)) where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections all sub-layers in the model as well as the embedding\\nlayers produce outputs of dimension dmodel \\n= 51'), start_index=2927, text='„Äê12‚Ä†source„Äë', type='file_citation')], value='The paper \"Attention is All You Need\" presents a novel network architecture termed the Transformer, which is based solely on attention mechanisms and excludes the use of recurrent and convolutional layers typically found in state-of-the-art sequence transduction models. The key insights and methodologies of the paper are as follows:\\n\\n**Abstract Insights:**\\n- Traditional sequence transduction models rely on complex encoder-decoder architectures with recurrent or convolutional networks, often enhanced with attention mechanisms for performance improvement.\\n- The Transformer innovates by utilizing only attention mechanisms, which results in superior performance in terms of quality, parallelization, and training time efficiency.\\n- Notable achievements include reaching a BLEU score of 28.4 on the WMT 2014 English-to-German translation task and 41.8 on the WMT 2014 English-to-French translation task, both of which are substantial improvements over prior state-of-the-art results.\\n- The Transformer also demonstrates its generalizability by being successfully applied to English constituency parsing tasks with both large and limited training data„Äê9‚Ä†source„Äë.\\n\\n**Introduction and Background:**\\n- Recurrent neural networks, especially LSTM and GRU, have been the gold standard in sequence modeling and machine translation.\\n- The inherent sequential nature of RNNs poses limitations for parallelization within training examples which is problematic for longer sequences due to batching constraints.\\n- Although attention mechanisms have been integrated within recurrent networks to capture dependencies regardless of distance in the sequences, they still carry the limitations of sequential processing.\\n- The Transformer architecture is the first to fully rely solely on self-attention to process its input and output without using sequence-aligned RNNs or convolution, marking a significant departure from previous approaches„Äê10‚Ä†source„Äë„Äê11‚Ä†source„Äë.\\n\\n**Model Architecture:**\\n- The Transformer adopts an encoder-decoder structure, where the encoder maps an input sequence to continuous representations, and the decoder, in an auto-regressive manner, generates the output sequence one symbol at a time, informed by the previously generated symbols.\\n- The encoder consists of six identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections and layer normalization are applied around each sub-layer.\\n- The decoder is also made of six identical layers and includes a third sub-layer for multi-head attention over the encoder output. Masking in the self-attention layers of the decoder makes sure that when predicting a symbol, only the previous symbols are used, ensuring proper auto-regression.\\n- The model uses multi-head attention to counteract the loss of resolution that comes from the averaging attention-weighted positions„Äê12‚Ä†source„Äë.\\n\\nOverall, the Transformer showed that models for sequence processing could achieve state-of-the-art performance without recurrent architectures, through a smart design that leverages parallelizable attention mechanisms to improve training efficiency and model quality. This architecture has significant implications for various sequence modeling tasks and has set new records in translation tasks with less training cost than previous models. The Transformer‚Äôs ability to generalize across different types of sequence-to-sequence tasks, such as constituency parsing, indicates its versatility and broad applicability.'), type='text')], created_at=1707561270, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_CfcGAncVeydIP0H6FxMJirEj', thread_id='thread_zBoSxtQS2bg1YfKfJYrKFmq6'), ThreadMessage(id='msg_VjEfDqRXz5NLu1doMBl9pAmB', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='\\n    Write a detailed summary of the input paper, synthesizing its key findings, methodologies, and \\n    implications, providing overview for reference and comprehension purposes.'), type='text')], created_at=1707561261, file_ids=['file-xztVmJZF96ktWcJQDNaD2pX2'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_zBoSxtQS2bg1YfKfJYrKFmq6')], object='list', first_id='msg_Ntd8pntQfbchrtwBxf5YubmV', last_id='msg_VjEfDqRXz5NLu1doMBl9pAmB', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "create_run_2 = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "retrieve_run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=create_run_2.id\n",
    ")\n",
    "\n",
    "responses = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ThreadMessage(id='msg_Ntd8pntQfbchrtwBxf5YubmV', assistant_id='asst_ilzVq2Zpu4cY9h4x0zA039BP', content=[MessageContentText(text=Text(annotations=[TextAnnotationFileCitation(end_index=1163, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='Abstract\\n\\n\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture the Transformer\\nbased solely on attention mechanisms dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task improving over the existing best results including\\nensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data'), start_index=1153, text='„Äê9‚Ä†source„Äë', type='file_citation'), TextAnnotationFileCitation(end_index=1939, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='1 Introduction\\n\\n\\nRecurrent neural networks long short-term memory [13] and gated recurrent [7] neural networks\\nin particular have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35 2 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38 24 15].\\n\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time they generate a sequence of hidden\\nstates ht\\n as a function of the previous hidden state ht‚àí1 \\nand the input for position t. This inherently\\nsequential nature precludes parallelization within training examples which becomes critical at longer\\nsequence lengths as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32] while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation however remains.\\n\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2 19]. In all but a few cases [27] however such attention mechanisms\\nare used in conjunction with a recurrent network.\\n\\n\\nIn this work we propose the Transformer a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n\\n2 Background\\n\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16] ByteNet [18] and ConvS2S [9] all of which use convolutional neural networks as basic building\\nblock computing hidden representations in parallel for all input and output positions. In these models\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n\\n\\nSelf-attention sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension abstractive summarization\\ntextual entailment and learning task-independent sentence representations [4 27 28 22].\\n\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\n\\n\\nTo the best of our knowledge however the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections we will describe the Transformer motivate\\nself-attention and discuss its advantages over models such as [17 18] and [9].\\n\\n\\n3 Model Architecture\\n\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5 2 35].\\nHere the encoder maps an input sequence of symbol representations (x1\\n ... xn\\n) to a sequence\\nof continuous representations z = (z1\\n ... zn\\n).  Given z the decoder then generates an output\\nsequence (y1\\n ... ym\\n) of symbols one element at a time. At each step the model is auto-regressive\\n[10] consuming the previously generated symbols as additional input when generating the next.\\n\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise fully\\nconnected layers for both the encoder and decoder shown in the left and right halves of Figure 1\\nrespectively.\\n\\n\\n3.1  Encoder and Decoder Stacks\\n\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism and the second is a simple position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers followed by layer normalization [1]. That is the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)) where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections all sub-layers in the model as well as the embedding\\nlayers produce outputs of dimension dmodel \\n= 512.\\n\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer the decoder inserts a third sub-layer which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder we employ residual connections\\naround each of the sub-layers followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking combined with fact that the output embeddings are offset by one position ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i'), start_index=1928, text='„Äê10‚Ä†source„Äë', type='file_citation'), TextAnnotationFileCitation(end_index=1950, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='2 Background\\n\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16] ByteNet [18] and ConvS2S [9] all of which use convolutional neural networks as basic building\\nblock computing hidden representations in parallel for all input and output positions. In these models\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2'), start_index=1939, text='„Äê11‚Ä†source„Äë', type='file_citation'), TextAnnotationFileCitation(end_index=2938, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-xztVmJZF96ktWcJQDNaD2pX2', quote='3 Model Architecture\\n\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5 2 35].\\nHere the encoder maps an input sequence of symbol representations (x1\\n ... xn\\n) to a sequence\\nof continuous representations z = (z1\\n ... zn\\n).  Given z the decoder then generates an output\\nsequence (y1\\n ... ym\\n) of symbols one element at a time. At each step the model is auto-regressive\\n[10] consuming the previously generated symbols as additional input when generating the next.\\n\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise fully\\nconnected layers for both the encoder and decoder shown in the left and right halves of Figure 1\\nrespectively.\\n\\n\\n3.1  Encoder and Decoder Stacks\\n\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism and the second is a simple position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers followed by layer normalization [1]. That is the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)) where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections all sub-layers in the model as well as the embedding\\nlayers produce outputs of dimension dmodel \\n= 51'), start_index=2927, text='„Äê12‚Ä†source„Äë', type='file_citation')], value='The paper \"Attention is All You Need\" presents a novel network architecture termed the Transformer, which is based solely on attention mechanisms and excludes the use of recurrent and convolutional layers typically found in state-of-the-art sequence transduction models. The key insights and methodologies of the paper are as follows:\\n\\n**Abstract Insights:**\\n- Traditional sequence transduction models rely on complex encoder-decoder architectures with recurrent or convolutional networks, often enhanced with attention mechanisms for performance improvement.\\n- The Transformer innovates by utilizing only attention mechanisms, which results in superior performance in terms of quality, parallelization, and training time efficiency.\\n- Notable achievements include reaching a BLEU score of 28.4 on the WMT 2014 English-to-German translation task and 41.8 on the WMT 2014 English-to-French translation task, both of which are substantial improvements over prior state-of-the-art results.\\n- The Transformer also demonstrates its generalizability by being successfully applied to English constituency parsing tasks with both large and limited training data„Äê9‚Ä†source„Äë.\\n\\n**Introduction and Background:**\\n- Recurrent neural networks, especially LSTM and GRU, have been the gold standard in sequence modeling and machine translation.\\n- The inherent sequential nature of RNNs poses limitations for parallelization within training examples which is problematic for longer sequences due to batching constraints.\\n- Although attention mechanisms have been integrated within recurrent networks to capture dependencies regardless of distance in the sequences, they still carry the limitations of sequential processing.\\n- The Transformer architecture is the first to fully rely solely on self-attention to process its input and output without using sequence-aligned RNNs or convolution, marking a significant departure from previous approaches„Äê10‚Ä†source„Äë„Äê11‚Ä†source„Äë.\\n\\n**Model Architecture:**\\n- The Transformer adopts an encoder-decoder structure, where the encoder maps an input sequence to continuous representations, and the decoder, in an auto-regressive manner, generates the output sequence one symbol at a time, informed by the previously generated symbols.\\n- The encoder consists of six identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections and layer normalization are applied around each sub-layer.\\n- The decoder is also made of six identical layers and includes a third sub-layer for multi-head attention over the encoder output. Masking in the self-attention layers of the decoder makes sure that when predicting a symbol, only the previous symbols are used, ensuring proper auto-regression.\\n- The model uses multi-head attention to counteract the loss of resolution that comes from the averaging attention-weighted positions„Äê12‚Ä†source„Äë.\\n\\nOverall, the Transformer showed that models for sequence processing could achieve state-of-the-art performance without recurrent architectures, through a smart design that leverages parallelizable attention mechanisms to improve training efficiency and model quality. This architecture has significant implications for various sequence modeling tasks and has set new records in translation tasks with less training cost than previous models. The Transformer‚Äôs ability to generalize across different types of sequence-to-sequence tasks, such as constituency parsing, indicates its versatility and broad applicability.'), type='text')], created_at=1707561270, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_CfcGAncVeydIP0H6FxMJirEj', thread_id='thread_zBoSxtQS2bg1YfKfJYrKFmq6'), ThreadMessage(id='msg_VjEfDqRXz5NLu1doMBl9pAmB', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='\\n    Write a detailed summary of the input paper, synthesizing its key findings, methodologies, and \\n    implications, providing overview for reference and comprehension purposes.'), type='text')], created_at=1707561261, file_ids=['file-xztVmJZF96ktWcJQDNaD2pX2'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_zBoSxtQS2bg1YfKfJYrKFmq6')]\n"
     ]
    }
   ],
   "source": [
    "print(responses.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To connect our agent to online resources we need some search engine as tools.\n",
    "# create the search tool based on new sholars\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(time=\"m\")\n",
    "tools = [DuckDuckGoSearchResults(api_wrapper=wrapper, source=\"scholar\")]\n",
    "\n",
    "# create the agent of assistant-openai\n",
    "agent = OpenAIAssistantRunnable(\n",
    "    tools=tools,\n",
    "    assistant_id=assistant.id,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    as_agent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, return_intermediate_steps=True)\n",
    "output = agent_executor.invoke({\"content\": online_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '\\n    Give structured JSON file for similar papers that you found contains: paper_title, authors_names, \\n    publication_date, link_of_paper and the conference_or_journal_name where it was published.\\n', 'output': 'Here is the requested information in JSON format for the similar papers:\\n\\n```json\\n[\\n    {\\n        \"paper_title\": \"End-to-end memory networks\",\\n        \"authors_names\": [\"Sainbayar Sukhbaatar\", \"Arthur Szlam\", \"Jason Weston\", \"Rob Fergus\"],\\n        \"publication_date\": \"2015\",\\n        \"link_of_paper\": \"https://papers.nips.cc/paper/5846-end-to-end-memory-networks\",\\n        \"conference_or_journal_name\": \"Advances in Neural Information Processing Systems 28 (NIPS 2015)\"\\n    },\\n    {\\n        \"paper_title\": \"Sequence to sequence learning with neural networks\",\\n        \"authors_names\": [\"Ilya Sutskever\", \"Oriol Vinyals\", \"Quoc VV Le\"],\\n        \"publication_date\": \"2014\",\\n        \"link_of_paper\": \"https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks\",\\n        \"conference_or_journal_name\": \"Advances in Neural Information Processing Systems (NIPS 2014)\"\\n    },\\n    {\\n        \"paper_title\": \"Rethinking the inception architecture for computer vision\",\\n        \"authors_names\": [\"Christian Szegedy\", \"Vincent Vanhoucke\", \"Sergey Ioffe\", \"Jonathon Shlens\", \"Zbigniew Wojna\"],\\n        \"publication_date\": \"2015\",\\n        \"link_of_paper\": \"https://arxiv.org/abs/1512.00567\",\\n        \"conference_or_journal_name\": \"CoRR (arXiv 1512.00567)\"\\n    },\\n    {\\n        \"paper_title\": \"Grammar as a foreign language\",\\n        \"authors_names\": [\"Oriol Vinyals\", \"≈Åukasz Kaiser\", \"Terry Koo\", \"Slav Petrov\", \"Ilya Sutskever\", \"Geoffrey Hinton\"],\\n        \"publication_date\": \"2015\",\\n        \"link_of_paper\": \"https://papers.nips.cc/paper/5635-grammar-as-a-foreign-language\",\\n        \"conference_or_journal_name\": \"Advances in Neural Information Processing Systems (NIPS 2015)\"\\n    },\\n    {\\n        \"paper_title\": \"Google\\'s neural machine translation system: Bridging the gap between human and machine translation\",\\n        \"authors_names\": [\"Yonghui Wu\", \"Mike Schuster\", \"Zhifeng Chen\", \"Quoc V Le\", \"Mohammad Norouzi\", \"Wolfgang Macherey\", \"Maxim Krikun\", \"Yuan Cao\", \"Qin Gao\", \"Klaus Macherey\", \"et al.\"],\\n        \"publication_date\": \"2016\",\\n        \"link_of_paper\": \"https://arxiv.org/abs/1609.08144\",\\n        \"conference_or_journal_name\": \"arXiv preprint (arXiv:1609.08144)\"\\n    },\\n    {\\n        \"paper_title\": \"Deep recurrent models with fast-forward connections for neural machine translation\",\\n        \"authors_names\": [\"Jie Zhou\", \"Ying Cao\", \"Xuguang Wang\", \"Peng Li\", \"Wei Xu\"],\\n        \"publication_date\": \"2016\",\\n        \"link_of_paper\": \"https://arxiv.org/abs/1606.04199\",\\n        \"conference_or_journal_name\": \"CoRR (arXiv 1606.04199)\"\\n    },\\n    {\\n        \"paper_title\": \"Fast and accurate shift-reduce constituent parsing\",\\n        \"authors_names\": [\"Muhua Zhu\", \"Yue Zhang\", \"Wenliang Chen\", \"Min Zhang\", \"Jingbo Zhu\"],\\n        \"publication_date\": \"2013\",\\n        \"link_of_paper\": \"https://www.aclweb.org/anthology/P13-1045\",\\n        \"conference_or_journal_name\": \"51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)\"\\n    }\\n]\\n```\\n\\nPlease note that the links provided for each paper are based on the common locations where such papers would be found (NIPS proceedings, arXiv, ACL Anthology). The actual links may differ if the documents have been moved or rehosted, so I recommend verifying the links before using them.', 'thread_id': 'thread_0xjO06mAWlNeMiMgHTULDExp', 'run_id': 'run_cuxfV269QjA04ZbJphVGROZR', 'intermediate_steps': []}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
